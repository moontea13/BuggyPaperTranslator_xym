title,authors,abstract,date,paper_url,score,title_cn,abstract_cn
Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts,Hongcheng Gao, Tianyu Pang, Chao Du, Taihang Hu, Zhijie Deng, Min Lin,With the rapid progress of diffusion models (DMs), significant efforts are being made to unlearn harmful or copyrighted concepts from pretrained DMs to prevent potential model misuse. However, it is observed that even when DMs are properly unlearned before release, malicious finetuning can compromise this process, causing DMs to relearn the unlearned concepts. This occurs partly because certain benign concepts (e.g., "skin") retained in DMs are related to the unlearned ones (e.g., "nudity"), facilitating their relearning via finetuning. To address this, we propose meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes malicious finetuning on unlearned concepts, the related benign concepts retained within it will be triggered to self-destruct, hindering the relearning of unlearned concepts. Our meta-unlearning framework is compatible with most existing unlearning methods, requiring only the addition of an easy-to-implement meta objective. We validate our approach through empirical experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4 and SDXL), supported by extensive ablation studies.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Meta-Unlearning_on_Diffusion_Models_Preventing_Relearning_Unlearned_Concepts_ICCV_2025_paper.html,0.99609375,扩散模型中的元遗忘：防止重新学习已遗忘的概念,随着扩散模型（DMs）的快速进展，越来越多的工作致力于从预训练的扩散模型中“忘记”有害或受版权保护的概念，以防止模型被滥用。然而观察到，即使在发布前对扩散模型进行了适当的忘记处理，恶意微调仍然可能破坏这一过程，使模型重新学习被忘记的概念。这部分是因为某些保留在模型中的良性概念（例如“皮肤”）与被忘记的概念（例如“裸露”）相关联，微调时会促使这些概念重新被学习。为了解决这一问题，我们提出了对扩散模型进行元忘记（meta-unlearning）。直观上，元忘记后的模型在直接使用时应表现得像已经忘记的模型；而如果对该元忘记模型进行针对被忘记概念的恶意微调，模型内部保留的相关良性概念将被触发自毁，从而阻碍被忘记概念的重新学习。我们的元忘记框架可以兼容大多数现有的忘记方法，只需额外加入一个易于实现的元目标。我们通过对 Stable Diffusion（SD‑v1‑4 和 SDXL）模型进行概念元忘记的实验证明了该方法的有效性，并辅以大量消融实验进行支持。
Your Text Encoder Can Be An Object-Level Watermarking Controller,Naresh Kumar Devulapally, Mingzhen Huang, Vishal Asnani, Shruti Agarwal, Siwei Lyu, Vishnu Suresh Lokhande,Invisible watermarking of AI-generated images can help with copyright protection, enabling detection and identification of AI-generated media. In this work, we present a novel approach to watermark images of T2I Latent Diffusion Models (LDMs). By only fine-tuning text token embeddings \mathcal W _*, we enable watermarking in selected objects or parts of the image, offering greater flexibility compared to traditional full-image watermarking. Our method leverages the text encoder's compatibility across various LDMs, allowing plug-and-play integration for different LDMs. Moreover, introducing the watermark early in the encoding stage improves robustness to adversarial perturbations in later stages of the pipeline. Our approach achieves 99% bit accuracy (48 bits) with a 10^5 xreduction in model parameters, enabling efficient watermarking.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Devulapally_Your_Text_Encoder_Can_Be_An_Object-Level_Watermarking_Controller_ICCV_2025_paper.html,0.96875,你的文本编码器可以成为对象级水印控制器,隐形水印技术用于AI生成图像可以帮助实现版权保护，能够检测和识别AI生成的媒体。在本工作中，我们提出了一种针对文本到图像潜在扩散模型（T2I Latent Diffusion Models，LDMs）图像的全新水印方法。仅通过微调文本标记嵌入 \mathcal W_*，我们实现了在图像的特定对象或局部区域进行水印嵌入，相较于传统的全图水印提供了更大的灵活性。该方法利用文本编码器在不同LDM之间的兼容性，实现了对各种LDM的即插即用集成。此外，在编码阶段早期引入水印能够提升对后续流水线中对抗扰动的鲁棒性。实验表明，我们的方案在48位水印上达到了99%的比特准确率，并且模型参数量降低了10^5倍，从而实现了高效的水印嵌入。
Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning,Saemi Moon, Minjong Lee, Sangdon Park, Dongwoo Kim,As text-to-image diffusion models gain widespread commercial applications, there are increasing concerns about unethical or harmful use, including the unauthorized generation of copyrighted or sensitive content. Concept unlearning has emerged as a promising solution to these challenges by removing undesired and harmful information from the pre-trained model. However, the previous evaluations primarily focus on whether target concepts are removed while preserving image quality, neglecting the broader impacts such as unintended side effects. In this work, we propose Holistic Unlearning Benchmark (HUB), a comprehensive framework for evaluating unlearning methods across six key dimensions: faithfulness, alignment, pinpoint-ness, multilingual robustness, attack robustness, and efficiency. Our benchmark covers 33 target concepts, including 16,000 prompts per concept, spanning four categories: Celebrity, Style, Intellectual Property, and NSFW. Our investigation reveals that no single method excels across all evaluation criteria. By releasing our evaluation code and dataset, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Moon_Holistic_Unlearning_Benchmark_A_Multi-Faceted_Evaluation_for_Text-to-Image_Diffusion_Model_ICCV_2025_paper.html,0.9609375,全方位遗忘基准：针对文本到图像扩散模型遗忘的多维评估,随着文本到图像扩散模型在商业领域的广泛应用，人们对其不道德或有害使用的担忧日益增加，包括未经授权生成受版权保护或敏感内容。概念消除（concept unlearning）已成为应对这些挑战的有前景的解决方案，通过从预训练模型中移除不期望的有害信息来实现。然而，以往的评估主要关注是否成功删除目标概念并保持图像质量，忽视了更广泛的影响，如意外的副作用。在本工作中，我们提出了全方位消除基准（Holistic Unlearning Benchmark，HUB），这是一个涵盖六个关键维度——忠实度、对齐度、精准度、多语言鲁棒性、攻击鲁棒性和效率——的综合评估框架。我们的基准覆盖33个目标概念，每个概念包含1.6万条提示，涉及名人、风格、知识产权和不安全内容四大类。研究发现，没有任何单一方法能够在所有评估指标上表现出色。通过公开我们的评估代码和数据集，我们希望激发该领域的进一步研究，推动更可靠、更有效的概念消除方法的发展。
MUNBa: Machine Unlearning via Nash Bargaining,Jing Wu, Mehrtash Harandi,Machine Unlearning (MU) aims to selectively erase harmful behaviors from models while retaining the overall utility of the model. As a multi-task learning problem, MU involves balancing objectives related to forgetting specific concepts/data and preserving general performance. A naive integration of these forgetting and preserving objectives can lead to gradient conflicts and dominance, impeding MU algorithms from reaching optimal solutions.To address the gradient conflict and dominance issue, we reformulate MU as a two-player cooperative game, where the two players, namely, the forgetting player and the preservation player, contribute via their gradient proposals to maximize their overall gain and balance their contributions.To this end, inspired by the Nash bargaining theory, we derive a closed-form solution to guide the model toward the Pareto stationary point.Our formulation of MU guarantees an equilibrium solution, where any deviation from the final state would lead to a reduction in the overall objectives for both players, ensuring optimality in each objective.We evaluate our algorithm's effectiveness on a diverse set of tasks across image classification and image generation.Extensive experiments with ResNet, vision-language model CLIP, and text-to-image diffusion models demonstrate that our method outperforms state-of-the-art MU algorithms, achieving a better trade-off between forgetting and preserving.Our results also highlight improvements in forgetting precision, preservation of generalization, and robustness against adversarial attacks.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Wu_MUNBa_Machine_Unlearning_via_Nash_Bargaining_ICCV_2025_paper.html,0.94140625,None,机器遗忘（MU）旨在有选择地从模型中擦除有害行为，同时保留模型的整体效用。作为一个多任务学习问题，MU 需要在忘记特定概念/数据和保持总体性能之间平衡目标。对这些忘记和保留目标的朴素整合会导致梯度冲突和支配现象，阻碍 MU 算法达到最优解。为了解决梯度冲突和支配问题，我们将 MU 重新表述为一个双人合作博弈，其中忘记玩家和保留玩家通过各自的梯度提议来最大化整体收益并平衡贡献。为此，受纳什议价理论的启发，我们推导出一个闭式解，引导模型朝向帕累托驻点。我们的 MU 形式化保证了一个均衡解，任何偏离最终状态的行为都会导致两位玩家的整体目标下降，从而确保每个目标的最优性。我们在图像分类和图像生成等多样任务上评估了算法的有效性。大量使用 ResNet、视觉语言模型 CLIP 以及文本到图像扩散模型的实验表明，我们的方法优于最新的 MU 算法，在忘记与保留之间实现了更好的权衡。结果还显示在忘记精度、泛化保持以及对抗攻击鲁棒性方面都有所提升。
Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks,Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yicheng Fu, Yichun Feng, Kin-Man Lam,Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Safeguarding_Vision-Language_Models_Mitigating_Vulnerabilities_to_Gaussian_Noise_in_Perturbation-based_ICCV_2025_paper.html,0.93359375,保障视觉语言模型：缓解基于扰动攻击中对高斯噪声的脆弱性,视觉语言模型（VLM）通过加入视觉信息扩展了大型语言模型（LLM）的能力，但它们仍然容易受到越狱攻击，尤其是在处理噪声或受损图像时。虽然现有的 VLM 在训练过程中采用了安全措施以减轻此类攻击，但对噪声增强视觉输入的脆弱性却被忽视。在本工作中，我们发现缺乏噪声增强训练导致了关键的安全漏洞：许多 VLM 对甚至简单的高斯噪声扰动都很敏感。为了解决这一挑战，我们提出了 Robust‑VLGuard，这是一套包含对齐/不对齐图文对的多模态安全数据集，并结合噪声增强微调，使攻击成功率降低的同时保持 VLM 的功能。针对更强的基于优化的视觉扰动攻击，我们提出了 DiffPure‑VLM，利用扩散模型将对抗扰动转化为类似高斯噪声，从而可以通过噪声增强的安全微调来防御。实验结果表明，扩散模型的分布迁移特性与我们微调后的 VLM 匹配良好，显著减轻了不同强度的对抗扰动。数据集和代码已在 https://github.com/JarvisUSTC/DiffPure-RobustVLM 上公开。
Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization,Gen Li, Yang Xiao, Jie Ji, Kaiyuan Deng, Bo Hui, Linke Guo, Xiaolong Ma,Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose **Dynamic Mask coupled with Concept-Aware Loss**, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our **Dynamic Mask** mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our **Concept-Aware Loss** explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. Code available in https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Li_Sculpting_Memory_Multi-Concept_Forgetting_in_Diffusion_Models_via_Dynamic_Mask_ICCV_2025_paper.html,0.92578125,雕刻记忆：通过动态掩码和概念感知优化实现扩散模型的多概念遗忘,文本到图像（T2I）扩散模型在根据文本提示生成高质量图像方面取得了显著成功。然而，它们能够存储大量知识的能力在需要选择性遗忘的场景中引发了担忧，例如删除受版权保护的内容、降低偏见或消除有害概念。虽然现有的遗忘方法可以去除某些概念，但在多概念遗忘方面仍面临不稳定、残留知识仍存以及生成质量下降等问题。为了解决这些挑战，我们提出了**动态掩码结合概念感知损失**的全新遗忘框架，专为扩散模型中的多概念遗忘设计。我们的**动态掩码**机制能够根据当前优化状态自适应更新梯度掩码，实现对权重的选择性修改，防止对无关知识产生干扰。此外，**概念感知损失**通过超类对齐强制语义一致性，显式引导遗忘过程，而基于知识蒸馏的正则化损失则确保在顺序遗忘时已遗忘的概念保持被遗忘。我们进行了大量实验来评估该方法。结果表明，在遗忘效果、输出保真度和语义连贯性方面，尤其是在多概念场景下，我们的方法优于现有的遗忘技术。我们的工作为生成模型提供了一个原理清晰、灵活且能够实现稳定高保真遗忘的框架。代码已在 https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025 发布。
PlugMark: A Plug-in Zero-Watermarking Framework for Diffusion Models,Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, Enci Liu, Zhuoyi Shang, Xiangyang Ji, Wu Liu,Diffusion models have significantly advanced the field of image synthesis, making the protection of their intellectual property (IP) a critical concern. Existing IP protection methods primarily focus on embedding watermarks into generated images by altering the structure of the diffusion process. However, these approaches inevitably compromise the quality of the generated images and are particularly vulnerable to fine-tuning attacks, especially for open-source models such as Stable Diffusion (SD). In this paper, we propose PlugMark, a novel plug-in zero-watermarking framework for diffusion models. The core idea of PlugMark is based on two observations: a classifier can be uniquely characterized by its decision boundaries, and a diffusion model can be uniquely represented by the knowledge acquired from training data.Building on this foundation, we introduce a diffusion knowledge extractor that can be plugged into a diffusion model to extract its knowledge and output a classification result. PlugMark subsequently generates boundary representations based on this classification result, serving as a zero-distortion watermark that uniquely represents the decision boundaries and, by extension, the knowledge of the diffusion model. Since only the extractor requires training, the performance of the original diffusion model remains unaffected.Extensive experimental results demonstrate that PlugMark can robustly extract high-confidence zero-watermarks from both the original model and its post-processed versions while effectively distinguishing them from non-post-processed diffusion models.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Chen_PlugMark_A_Plug-in_Zero-Watermarking_Framework_for_Diffusion_Models_ICCV_2025_paper.html,0.92578125,PlugMark：一种用于扩散模型的插件式零水印框架,扩散模型显著推动了图像合成领域的发展，使其知识产权（IP）保护成为关键问题。现有的IP保护方法主要通过改变扩散过程的结构在生成图像中嵌入水印，但这些方法不可避免地会降低生成图像的质量，并且在面对微调攻击时尤其脆弱，尤其是针对像Stable Diffusion（SD）这样的开源模型。本文提出了PlugMark，一种面向扩散模型的全新插件式零水印框架。PlugMark的核心思路基于两个观察：分类器可以通过其决策边界唯一表征，扩散模型可以通过从训练数据中获取的知识唯一表示。基于此，我们引入了一个扩散知识提取器，可插入扩散模型以提取其知识并输出分类结果。PlugMark随后根据该分类结果生成边界表示，作为零失真水印，唯一地代表决策边界，进而代表扩散模型的知识。由于仅需对提取器进行训练，原始扩散模型的性能保持不受影响。大量实验结果表明，PlugMark能够稳健地从原始模型及其后处理版本中提取高置信度的零水印，并能够有效地区分未经过后处理的扩散模型。
ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models,Hyun Jun Yook, Ga San Jhun, Jae Hyun Cho, Min Jeon, Donghyun Kim, Tae Hyung Kim, Youn Kyu Lee,Machine unlearning (MU) removes specific data points or concepts from deep learning models to enhance privacy and prevent sensitive content generation. Adversarial prompts can exploit unlearned models to generate content containing removed concepts, posing a significant security risk. However, existing adversarial attack methods still face challenges in generating content that aligns with an attacker's intent while incurring high computational costs to identify successful prompts. To address these challenges, we propose ZIUM, a Zero-shot Intent-aware adversarial attack on Unlearned Models, which enables the flexible customization of target attack images to reflect an attacker's intent. Additionally, ZIUM supports zero-shot adversarial attacks without requiring further optimization for previously attacked unlearned concepts. The evaluation across various MU scenarios demonstrated ZIUM's effectiveness in successfully customizing content based on user-intent prompts while achieving a superior attack success rate compared to existing methods. Moreover, its zero-shot adversarial attack significantly reduces the attack time for previously attacked unlearned concepts.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Yook_ZIUM_Zero-Shot_Intent-Aware_Adversarial_Attack_on_Unlearned_Models_ICCV_2025_paper.html,0.92578125,ZIUM：针对未学习模型的零样本意图感知对抗攻击,机器遗忘（MU）通过从深度学习模型中移除特定数据点或概念来提升隐私保护并防止生成敏感内容。对抗性提示可以利用已遗忘的模型生成包含被移除概念的内容，构成重大安全风险。然而，现有的对抗攻击方法在生成符合攻击者意图的内容时仍面临挑战，并且需要高昂的计算成本来寻找成功的提示。为了解决这些问题，我们提出了 ZIUM，即针对已遗忘模型的零样本意图感知对抗攻击，它能够灵活定制目标攻击图像以体现攻击者的意图。此外，ZIUM 支持零样本对抗攻击，无需对已攻击过的遗忘概念进行额外优化。对多种 MU 场景的评估表明，ZIUM 在成功根据用户意图提示定制内容方面表现出色，攻击成功率也优于现有方法。此外，其零样本对抗攻击显著缩短了对已攻击遗忘概念的攻击时间。
SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders,Jiahui Geng, Qing Li,Unlearning methods for vision-language models (VLMs) have primarily adapted techniques from large language models (LLMs), relying on weight updates that demand extensive annotated forget sets. Moreover, these methods perform unlearning at a coarse granularity, often leading to excessive forgetting and reduced model utility. To address this issue, we introduce SAUCE, a novel method that leverages sparse autoencoders (SAEs) for fine-grained and selective concept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture high-dimensional, semantically rich sparse features. It then identifies the features most relevant to the target concept for unlearning. During inference, it selectively modifies these features to suppress specific concepts while preserving unrelated information. We evaluate SAUCE on two distinct VLMs, LLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks: concrete concept unlearning (objects and sports scenes) and abstract concept unlearning (emotions, colors, and materials), encompassing a total of 60 concepts. Extensive experiments demonstrate that SAUCE outperforms state-of-the-art methods by 18.04% in unlearning quality while maintaining comparable model utility. Furthermore, we investigate SAUCE's robustness against widely used adversarial attacks, its transferability across models, and its scalability in handling multiple simultaneous unlearning requests. Our findings establish SAUCE as an effective and scalable solution for selective concept unlearning in VLMs.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Geng_SAUCE_Selective_Concept_Unlearning_in_Vision-Language_Models_with_Sparse_Autoencoders_ICCV_2025_paper.html,0.9140625,None,视觉语言模型（VLM） 的去学习方法主要借鉴了大型语言模型（LLM）的技术，依赖于需要大量标注遗忘集合的权重更新。此外，这些方法在粗粒度上进行去学习，往往导致过度遗忘并降低模型效用。为了解决这一问题，我们提出了 SAUCE，这是一种利用稀疏自编码器（SAE）实现细粒度、选择性概念去学习的创新方法。简而言之，SAUCE 首先训练 SAE 以捕获高维、语义丰富的稀疏特征，然后识别与待去学习目标概念最相关的特征。在推理阶段，它有选择地修改这些特征，以抑制特定概念，同时保留无关信息。我们在两种不同的 VLM（LLaVA‑v1.5‑7B 和 LLaMA‑3.2‑11B‑Vision‑Instruct）上进行评估，涉及两类任务：具体概念去学习（对象和体育场景）和抽象概念去学习（情感、颜色和材料），共计 60 个概念。大量实验表明，SAUCE 在去学习质量上比最先进的方法提升了 18.04%，且保持了相当的模型效用。此外，我们还研究了 SAUCE 对常用对抗攻击的鲁棒性、跨模型的可迁移性以及处理多重同步去学习请求的可扩展性。研究结果表明，SAUCE 是一种在 VLM 中实现选择性概念去学习的有效且可扩展的解决方案。
DADet: Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection,Hongwei Yu, Xinlong Ding, Jiawei Li, Jinlong Wang, Yudong Zhang, Rongquan Wang, Huimin Ma, Jiansheng Chen,While image conditional diffusion models demonstrate impressive generation capabilities, they exhibit high vulnerability when facing backdoor and adversarial attacks. In this paper, we define a scenario named diffusion anomaly where the generated results of a reverse process under attack deviate significantly from the normal ones. By analyzing the underlying formation mechanism of the diffusion anomaly, we reveal how perturbations are amplified during the reverse process and accumulated in the results. Based on the analysis, we reveal the phenomena of divergence and homogeneity, which cause the diffusion process to deviate significantly from the normal process and to decline in diversity. Leveraging these two phenomena, we propose a method named Diffusion Anomaly Detection (DADet) to effectively detect both backdoor and adversarial attacks. Extensive experiments demonstrate that our proposal achieves excellent defense performance against backdoor and adversarial attacks. Specifically, for the backdoor attack detection, our method achieves an F1 score of 99% on different datasets, including MS COCO and CIFAR-10. For the detection of adversarial samples, the F1 score exceeds 84% across three adversarial attacks and two different tasks, evaluated on the MS COCO and Places365 datasets, respectively.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor_ICCV_2025_paper.html,0.90625,DADet：通过扩散异常检测保护图像条件扩散模型免受对抗攻击和后门攻击,None
TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models,Ruidong Chen, Honglin Guo, Lanjun Wang, Chenyu Zhang, Weizhi Nie, An-An Liu,Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Chen_TRCE_Towards_Reliable_Malicious_Concept_Erasure_in_Text-to-Image_Diffusion_Models_ICCV_2025_paper.html,0.87890625,None,None
SEAL: Semantic Aware Image Watermarking,Kasra Arabi, R. Teal Witter, Chinmay Hegde, Niv Cohen,Generative models have rapidly evolved to generate realistic outputs. However, their synthetic outputs increasingly challenge the clear distinction between natural and AI-generated content, necessitating robust watermarking techniques to mark synthetic images. Watermarks are typically expected to preserve the integrity of the target image, withstand removal attempts, and prevent unauthorized insertion of the watermark pattern onto unrelated images. To address this need, recent methods embed persistent watermarks into images produced by diffusion models using the initial noise of the diffusion process. Yet, to do so, they either distort the distribution of generated images or require searching a large dictionary of candidate noise patterns for detection. In this paper, we propose a novel watermarking method that embeds semantic information about the generated image into the noise pattern, enabling a distortion-free watermark that can be verified without requiring a database of key patterns. Instead, the key pattern can be inferred from the semantic embedding of the image using locality-sensitive hashing. Furthermore, conditioning the watermark detection on the original image content improves its robustness against forgery attacks. To demonstrate that, we consider two largely overlooked attack strategies: (i) an attacker extracting the initial noise and generating a novel image with the same pattern; (ii) an attacker inserting an unrelated (potentially harmful) object into a watermarked image, while preserving the watermark. We empirically validate our method's increased robustness to these attacks. Taken together, our results suggest that content-aware watermarks can mitigate risks arising from image-generative models.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Arabi_SEAL_Semantic_Aware_Image_Watermarking_ICCV_2025_paper.html,0.87890625,None,None
Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design,Yuhao Sun, Yihua Zhang, Gaowen Liu, Hongtao Xie, Sijia Liu,With the increasing demand for the right to be forgotten, machine unlearning (MU) has emerged as a vital tool for enhancing trust and regulatory compliance by enabling the removal of sensitive data influences from machine learning (ML) models. However, most MU algorithms primarily rely on in-training methods to adjust model weights, with limited exploration of the benefits that data-level adjustments could bring to the unlearning process. To address this gap, we propose a novel approach that leverages digital watermarking to facilitate MU by strategically modifying data content. By integrating watermarking, we establish a controlled unlearning mechanism that enables precise removal of specified data while maintaining model utility for unrelated tasks. We first examine the impact of watermarked data on MU, finding that MU effectively generalizes to watermarked data. Building on this, we introduce an unlearning-friendly watermarking framework, termed Water4MU, to enhance unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO) framework: at the upper level, the watermarking network is optimized to minimize unlearning difficulty, while at the lower level, the model itself is trained independently of watermarking. Experimental results demonstrate that Water4MU is effective in MU across both image classification and image generation tasks. Notably, it outperforms existing methods in challenging MU scenarios, known as "challenging forgets".,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Invisible_Watermarks_Visible_Gains_Steering_Machine_Unlearning_with_Bi-Level_Watermarking_ICCV_2025_paper.html,0.8671875,None,None
Unlearning the Noisy Correspondence Makes CLIP More Robust,Haochen Han, Alex Jinpeng Wang, Peijun Ye, Fangming Liu,The data appetite for Vision-Language Models (VLMs) has continuously scaled up from the early millions to billions today, which faces an untenable trade-off with data quality and inevitably introduces Noisy Correspondence (NC) samples. Undoubtedly, such semantically unrelated data significantly impairs the performance of VLMs. Previous efforts mainly address this challenge by estimating refined alignment for more precise guidance. However, such resource-intensive pipelines that train VLMs from scratch struggle to meet realistic data demands. In this paper, we present a brand new perspective that seeks to directly eliminate the harmful effects of NC in pre-trained VLMs. Specifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning framework that efficiently enhances VLMs' robustness by forgetting learned noisy knowledge. The key to NCU is learning the hardest negative information, which can provide explicit unlearning direction for both false positives and false negatives. Such twin goals unlearning process can be formalized into one unified optimal transport objective for fast fine-tuning. We validate our approach with the prevailing CLIP model over various downstream tasks. Remarkably, NCU surpasses the robust pre-trained method on zero-shot transfer while with lower computational overhead. The code is available at https://github.com/hhc1997/NCU.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Han_Unlearning_the_Noisy_Correspondence_Makes_CLIP_More_Robust_ICCV_2025_paper.html,0.8359375,None,None
ROAR: Reducing Inversion Error in Generative Image Watermarking,Hanyi Wang, Han Fang, Shi-Lin Wang, Ee-Chien Chang,Generative image watermarking enables the proactive detection and traceability of generated images. Among existing methods, inversion-based frameworks achieve highly conceal ed watermark embedding by injecting watermarks into the latent representation before the diffusion process. The robustness of this approach hinges on both the embedding mechanism and inversion accuracy. However, prior works have predominantly focused on optimizing the embedding process while overlooking inversion errors, which significantly affect extraction fidelity. In this paper, we address the challenge of inversion errors and propose ROAR, a dual-domain optimization-based framework designed to mitigate errors arising from two key sources: 1) Latent-domain errors, which accumulate across inversion steps due to inherent approximation assumptions. 2) Pixel-domain errors, which result from channel distortions such as JPEG compression. To tackle these issues, we introduce two novel components: A Regeneration-based Optimization (RO) mechanism, which incorporates an optimizable starting latent to minimize latent-domain errors; A Mixture of Experts (MoE)-based distortion-adaptive restoration (AR) network, which effectively recovers watermarked distributions from pixel-level distortions.Extensive experiments demonstrate that ROAR significantly reduces inversion errors and enhances watermark extraction robustness, thereby improving the reliability of generative image watermarking.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking_ICCV_2025_paper.html,0.8359375,None,None
DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models,Seunghoo Hong, Geonho Son, Juhun Lee, Simon S. Woo,Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive performance. In this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community. Our code is available here: https://anonymous.4open.science/r/DIA-13419/.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Hong_DIA_The_Adversarial_Exposure_of_Deterministic_Inversion_in_Diffusion_Models_ICCV_2025_paper.html,0.796875,None,None
TrustMark: Robust Watermarking and Watermark Removal for Arbitrary Resolution Images,Tu Bui, Shruti Agarwal, John Collomosse,Imperceptible digital watermarking is important in copyright protection, misinformation prevention, and responsible generative AI. We propose TrustMark - a watermarking method that leverages a spatio-spectral loss function and a 1x1 convolution layer to enhance encoding quality. TrustMark is robust against both in-place and out-of-place perturbations while maintaining image quality above 43 dB. Additionally, we propose ReMark, a watermark removal method designed for re-watermarking, along with a simple yet effective algorithm that enables both TrustMark and ReMark to operate across arbitrary resolutions. Our methods achieve state-of-art performance on 3 benchmarks.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Bui_TrustMark_Robust_Watermarking_and_Watermark_Removal_for_Arbitrary_Resolution_Images_ICCV_2025_paper.html,0.796875,None,None
Moderating the Generalization of Score-based Generative Model,Wan Jiang, He Wang, Xin Zhang, Dan Guo, Zhaoxin Fan, Yunfeng Diao, Richang Hong,Score-based Generative Models (SGMs) have demonstrated remarkable generalization capabilities, e.g. generating unseen, but natural data. However, the greater the generalization power, the more likely the unintended generalization, and the more dangerous the abuse. Despite these concerns, research on unlearning SGMs has not been explored. To fill this gap, we first examine the current `gold standard' in Machine Unlearning (MU), i.e., re-training the model after removing the undesirable training data, and find it does not work in SGMs. Further analysis of score functions reveals that the MU 'gold standard' does not alter the original score function, which explains its ineffectiveness. Building on this insight, we propose the first Moderated Score-based Generative Model (MSGM), which introduces a novel score adjustment strategy that redirects the score function away from undesirable data during the continuous-time stochastic differential equation process. Albeit designed for SGMs, MSGM is a general and flexible MU framework compatible with diverse diffusion architectures, training strategies and downstream tasks. Code is available at https://github.com/yunfengdiao/Moderated-Score-based-Generative-Model.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Moderating_the_Generalization_of_Score-based_Generative_Model_ICCV_2025_paper.html,0.796875,None,None
AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs,Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha,With the rapid advancement of Multi-modal Large Language Models (MLLMs), several diagnostic benchmarks have recently been developed to assess these models' multimodal reasoning proficiency. However, these benchmarks are restricted to assessing primarily the visual aspect and do not examine the holistic audio-visual (AV) understanding. Moreover, currently, there are no benchmarks that investigate the capabilities of AVLLMs to calibrate their responses when presented with perturbed inputs. To this end, we introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial Attack, Compositional Reasoning, and Modality-specific Dependency. Using our benchmark, we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.,2025,https://openaccess.thecvf.com/content/ICCV2025/html/Chowdhury_AVTrustBench_Assessing_and_Enhancing_Reliability_and_Robustness_in_Audio-Visual_LLMs_ICCV_2025_paper.html,0.796875,None,None
