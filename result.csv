title,authors,abstract,date,paper_url,score,title_cn,abstract_cn
Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts,"Hongcheng Gao, Tianyu Pang, Chao Du, Taihang Hu, Zhijie Deng, Min Lin","With the rapid progress of diffusion models (DMs), significant efforts are being made to unlearn harmful or copyrighted concepts from pretrained DMs to prevent potential model misuse. However, it is observed that even when DMs are properly unlearned before release, malicious finetuning can compromise this process, causing DMs to relearn the unlearned concepts. This occurs partly because certain benign concepts (e.g., ""skin"") retained in DMs are related to the unlearned ones (e.g., ""nudity""), facilitating their relearning via finetuning. To address this, we propose meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes malicious finetuning on unlearned concepts, the related benign concepts retained within it will be triggered to self-destruct, hindering the relearning of unlearned concepts. Our meta-unlearning framework is compatible with most existing unlearning methods, requiring only the addition of an easy-to-implement meta objective. We validate our approach through empirical experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4 and SDXL), supported by extensive ablation studies.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Meta-Unlearning_on_Diffusion_Models_Preventing_Relearning_Unlearned_Concepts_ICCV_2025_paper.html,0.99609375,扩散模型的元遗忘：防止对已遗忘概念的重新学习,随着扩散模型（DMs）的快速发展，越来越多的工作致力于从预训练的扩散模型中“忘记”有害或受版权保护的概念，以防止模型被滥用。然而观察到，即使在发布前对扩散模型进行了适当的忘记处理，恶意微调仍然可能破坏这一过程，使模型重新学习被忘记的概念。这部分是因为某些保留在模型中的良性概念（例如“皮肤”）与被忘记的概念（例如“裸露”）相关联，微调时会促使这些良性概念帮助重新学习被忘记的内容。为了解决这一问题，我们提出了对扩散模型进行元忘记（meta‑unlearning）。直观上，元忘记后的模型在直接使用时应表现得像已经忘记的模型；而如果对其进行针对被忘记概念的恶意微调，模型内部保留的相关良性概念将被触发自毁，从而阻碍被忘记概念的重新学习。我们的元忘记框架可以兼容大多数现有的忘记方法，只需额外加入一个易于实现的元目标。我们通过在 Stable Diffusion（SD‑v1‑4 和 SDXL）模型上进行概念元忘记的实验验证了该方法，并辅以大量消融实验进行支持。
Your Text Encoder Can Be An Object-Level Watermarking Controller,"Naresh Kumar Devulapally, Mingzhen Huang, Vishal Asnani, Shruti Agarwal, Siwei Lyu, Vishnu Suresh Lokhande","Invisible watermarking of AI-generated images can help with copyright protection, enabling detection and identification of AI-generated media. In this work, we present a novel approach to watermark images of T2I Latent Diffusion Models (LDMs). By only fine-tuning text token embeddings \mathcal W _*, we enable watermarking in selected objects or parts of the image, offering greater flexibility compared to traditional full-image watermarking. Our method leverages the text encoder's compatibility across various LDMs, allowing plug-and-play integration for different LDMs. Moreover, introducing the watermark early in the encoding stage improves robustness to adversarial perturbations in later stages of the pipeline. Our approach achieves 99% bit accuracy (48 bits) with a 10^5 xreduction in model parameters, enabling efficient watermarking.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Devulapally_Your_Text_Encoder_Can_Be_An_Object-Level_Watermarking_Controller_ICCV_2025_paper.html,0.96875,您的文本编码器可以作为对象级水印控制器,隐形水印技术用于AI生成图像可以帮助版权保护，实现对AI生成媒体的检测和识别。在本工作中，我们提出了一种针对文本到图像潜在扩散模型（LDM）图像的全新水印方法。仅通过微调文本标记嵌入 \mathcal W _*，即可在图像的特定对象或局部区域实现水印，相较于传统的全图水印提供了更大的灵活性。该方法利用文本编码器在不同LDM之间的兼容性，实现了对各种LDM的即插即用集成。此外，在编码阶段早期引入水印能够提升对后续流水线中对抗扰动的鲁棒性。我们的方案在仅使用10^5倍参数量缩减的情况下，实现了99%比特准确率（48比特），从而实现高效的水印嵌入。
Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning,"Saemi Moon, Minjong Lee, Sangdon Park, Dongwoo Kim","As text-to-image diffusion models gain widespread commercial applications, there are increasing concerns about unethical or harmful use, including the unauthorized generation of copyrighted or sensitive content. Concept unlearning has emerged as a promising solution to these challenges by removing undesired and harmful information from the pre-trained model. However, the previous evaluations primarily focus on whether target concepts are removed while preserving image quality, neglecting the broader impacts such as unintended side effects. In this work, we propose Holistic Unlearning Benchmark (HUB), a comprehensive framework for evaluating unlearning methods across six key dimensions: faithfulness, alignment, pinpoint-ness, multilingual robustness, attack robustness, and efficiency. Our benchmark covers 33 target concepts, including 16,000 prompts per concept, spanning four categories: Celebrity, Style, Intellectual Property, and NSFW. Our investigation reveals that no single method excels across all evaluation criteria. By releasing our evaluation code and dataset, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Moon_Holistic_Unlearning_Benchmark_A_Multi-Faceted_Evaluation_for_Text-to-Image_Diffusion_Model_ICCV_2025_paper.html,0.9609375,全方位遗忘基准：针对文本到图像扩散模型遗忘的多维评估,随着文本到图像扩散模型在商业领域的广泛应用，人们对其不道德或有害使用的担忧日益增加，包括未经授权生成受版权保护或敏感内容。概念消除（concept unlearning）已成为应对这些挑战的有前景的解决方案，通过从预训练模型中移除不期望的有害信息。然而，以往的评估主要关注是否成功删除目标概念并保持图像质量，忽视了更广泛的影响，如意外的副作用。在本工作中，我们提出了全方位消除基准（Holistic Unlearning Benchmark，HUB），这是一个涵盖六个关键维度——忠实度、对齐度、精准度、多语言鲁棒性、攻击鲁棒性和效率——的综合评估框架。我们的基准覆盖33个目标概念，每个概念包含约1.6万条提示，涉及四大类：名人、风格、知识产权和不安全内容（NSFW）。研究发现，没有任何单一方法能够在所有评估指标上表现最佳。通过公开我们的评估代码和数据集，我们希望激发该领域的进一步研究，推动更可靠、更有效的概念消除方法的发展。
MUNBa: Machine Unlearning via Nash Bargaining,"Jing Wu, Mehrtash Harandi","Machine Unlearning (MU) aims to selectively erase harmful behaviors from models while retaining the overall utility of the model. As a multi-task learning problem, MU involves balancing objectives related to forgetting specific concepts/data and preserving general performance. A naive integration of these forgetting and preserving objectives can lead to gradient conflicts and dominance, impeding MU algorithms from reaching optimal solutions.To address the gradient conflict and dominance issue, we reformulate MU as a two-player cooperative game, where the two players, namely, the forgetting player and the preservation player, contribute via their gradient proposals to maximize their overall gain and balance their contributions.To this end, inspired by the Nash bargaining theory, we derive a closed-form solution to guide the model toward the Pareto stationary point.Our formulation of MU guarantees an equilibrium solution, where any deviation from the final state would lead to a reduction in the overall objectives for both players, ensuring optimality in each objective.We evaluate our algorithm's effectiveness on a diverse set of tasks across image classification and image generation.Extensive experiments with ResNet, vision-language model CLIP, and text-to-image diffusion models demonstrate that our method outperforms state-of-the-art MU algorithms, achieving a better trade-off between forgetting and preserving.Our results also highlight improvements in forgetting precision, preservation of generalization, and robustness against adversarial attacks.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Wu_MUNBa_Machine_Unlearning_via_Nash_Bargaining_ICCV_2025_paper.html,0.94140625,MUNBa：,机器遗忘（MU）旨在有选择地从模型中抹除有害行为，同时保留模型的整体效用。作为一个多任务学习问题，MU 需要在忘记特定概念/数据和保持整体性能之间进行平衡。对这些忘记和保留目标的简单整合会导致梯度冲突和支配现象，阻碍 MU 算法达到最优解。为了解决梯度冲突和支配问题，我们将 MU 重新表述为一个双人合作博弈，其中忘记玩家和保留玩家通过各自的梯度提案来最大化整体收益并平衡贡献。基于此，受纳什议价理论启发，我们推导出一个闭式解，引导模型趋向帕累托平稳点。我们的 MU 形式化保证了一个均衡解，任何偏离最终状态的行为都会导致两位玩家的整体目标下降，从而确保每个目标的最优性。我们在图像分类和图像生成等多样化任务上评估了算法的有效性。大量使用 ResNet、视觉语言模型 CLIP 以及文本到图像扩散模型的实验表明，我们的方法优于最先进的 MU 算法，实现了忘记与保留之间更好的权衡。实验结果还凸显了在忘记精度、泛化保留以及对抗攻击鲁棒性方面的提升。
Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks,"Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yicheng Fu, Yichun Feng, Kin-Man Lam","Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Safeguarding_Vision-Language_Models_Mitigating_Vulnerabilities_to_Gaussian_Noise_in_Perturbation-based_ICCV_2025_paper.html,0.93359375,保障视觉语言模型：缓解基于扰动攻击中对高斯噪声的脆弱性,视觉语言模型（VLM）通过加入视觉信息扩展了大型语言模型（LLM）的能力，但它们仍然容易受到越狱攻击，尤其是在处理噪声或受损图像时。虽然现有的 VLM 在训练过程中采用了安全措施来减轻此类攻击，但对噪声增强视觉输入的脆弱性却被忽视。在本工作中，我们发现缺乏噪声增强训练导致了关键的安全漏洞：许多 VLM 对甚至简单的扰动如高斯噪声都很敏感。为了解决这一挑战，我们提出了 Robust‑VLGuard，这是一套包含对齐/不对齐图文对的多模态安全数据集，并结合噪声增强的微调，使攻击成功率下降的同时保持 VLM 的功能。针对更强的基于优化的视觉扰动攻击，我们提出了 DiffPure‑VLM，利用扩散模型将对抗扰动转换为类似高斯噪声，从而可以通过噪声增强的安全微调来防御。实验结果表明，扩散模型的分布迁移特性与我们微调后的 VLM 匹配良好，显著减轻了不同强度的对抗扰动。数据集和代码已在 https://github.com/JarvisUSTC/DiffPure-RobustVLM 上公开。
Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization,"Gen Li, Yang Xiao, Jie Ji, Kaiyuan Deng, Bo Hui, Linke Guo, Xiaolong Ma","Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose **Dynamic Mask coupled with Concept-Aware Loss**, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our **Dynamic Mask** mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our **Concept-Aware Loss** explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. Code available in https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Li_Sculpting_Memory_Multi-Concept_Forgetting_in_Diffusion_Models_via_Dynamic_Mask_ICCV_2025_paper.html,0.92578125,雕刻记忆：通过动态掩码和概念感知优化实现扩散模型的多概念遗忘,文本到图像（T2I）扩散模型在根据文本提示生成高质量图像方面取得了显著成功。然而，它们能够存储大量知识的能力在需要选择性遗忘的场景中引发了担忧，例如删除受版权保护的内容、降低偏见或消除有害概念。虽然现有的遗忘方法可以去除某些概念，但在多概念遗忘方面仍面临不稳定、残留知识持续存在以及生成质量下降等问题。为了解决这些挑战，我们提出了**动态掩码结合概念感知损失**的全新遗忘框架，专为扩散模型中的多概念遗忘而设计。我们的**动态掩码**机制能够根据当前优化状态自适应更新梯度掩码，实现对权重的选择性修改，从而避免对无关知识的干扰。此外，我们的**概念感知损失**通过超类对齐强制语义一致性，明确引导遗忘过程；基于知识蒸馏的正则化损失则确保在顺序遗忘时已遗忘的概念仍保持被遗忘。我们进行了大量实验来评估我们的方法。结果表明，在遗忘效果、输出保真度和语义连贯性方面，尤其是在多概念场景下，我们的方法优于现有的遗忘技术。我们的工作为生成模型提供了一个原理清晰、灵活且能够实现稳定高保真遗忘的框架。代码已在 https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025 上公开。
PlugMark: A Plug-in Zero-Watermarking Framework for Diffusion Models,"Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, Enci Liu, Zhuoyi Shang, Xiangyang Ji, Wu Liu","Diffusion models have significantly advanced the field of image synthesis, making the protection of their intellectual property (IP) a critical concern. Existing IP protection methods primarily focus on embedding watermarks into generated images by altering the structure of the diffusion process. However, these approaches inevitably compromise the quality of the generated images and are particularly vulnerable to fine-tuning attacks, especially for open-source models such as Stable Diffusion (SD). In this paper, we propose PlugMark, a novel plug-in zero-watermarking framework for diffusion models. The core idea of PlugMark is based on two observations: a classifier can be uniquely characterized by its decision boundaries, and a diffusion model can be uniquely represented by the knowledge acquired from training data.Building on this foundation, we introduce a diffusion knowledge extractor that can be plugged into a diffusion model to extract its knowledge and output a classification result. PlugMark subsequently generates boundary representations based on this classification result, serving as a zero-distortion watermark that uniquely represents the decision boundaries and, by extension, the knowledge of the diffusion model. Since only the extractor requires training, the performance of the original diffusion model remains unaffected.Extensive experimental results demonstrate that PlugMark can robustly extract high-confidence zero-watermarks from both the original model and its post-processed versions while effectively distinguishing them from non-post-processed diffusion models.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Chen_PlugMark_A_Plug-in_Zero-Watermarking_Framework_for_Diffusion_Models_ICCV_2025_paper.html,0.92578125,PlugMark：一种用于扩散模型的插件式零水印框架,扩散模型在图像合成领域取得了显著进展，使得对其知识产权（IP）的保护成为关键问题。现有的IP保护方法主要通过改变扩散过程的结构在生成图像中嵌入水印，但这些方法不可避免地会降低生成图像的质量，并且在面对微调攻击时尤其脆弱，尤其是对开源模型如Stable Diffusion（SD）更是如此。本文提出了PlugMark，一种新颖的插件式零水印框架用于扩散模型。PlugMark的核心思想基于两个观察：分类器可以通过其决策边界唯一表征，扩散模型可以通过从训练数据中获得的知识唯一表示。基于此，我们引入了一个扩散知识提取器，可插入扩散模型以提取其知识并输出分类结果。PlugMark随后根据该分类结果生成边界表示，作为零失真水印，唯一地代表决策边界，进而代表扩散模型的知识。由于仅需对提取器进行训练，原始扩散模型的性能不受影响。大量实验结果表明，PlugMark能够稳健地从原始模型及其后处理版本中提取高置信度的零水印，并能够有效地区分这些模型与未经过后处理的扩散模型。
ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models,"Hyun Jun Yook, Ga San Jhun, Jae Hyun Cho, Min Jeon, Donghyun Kim, Tae Hyung Kim, Youn Kyu Lee","Machine unlearning (MU) removes specific data points or concepts from deep learning models to enhance privacy and prevent sensitive content generation. Adversarial prompts can exploit unlearned models to generate content containing removed concepts, posing a significant security risk. However, existing adversarial attack methods still face challenges in generating content that aligns with an attacker's intent while incurring high computational costs to identify successful prompts. To address these challenges, we propose ZIUM, a Zero-shot Intent-aware adversarial attack on Unlearned Models, which enables the flexible customization of target attack images to reflect an attacker's intent. Additionally, ZIUM supports zero-shot adversarial attacks without requiring further optimization for previously attacked unlearned concepts. The evaluation across various MU scenarios demonstrated ZIUM's effectiveness in successfully customizing content based on user-intent prompts while achieving a superior attack success rate compared to existing methods. Moreover, its zero-shot adversarial attack significantly reduces the attack time for previously attacked unlearned concepts.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Yook_ZIUM_Zero-Shot_Intent-Aware_Adversarial_Attack_on_Unlearned_Models_ICCV_2025_paper.html,0.92578125,ZIUM：对未学习模型的零样本意图感知对抗攻击,机器忘记（MU）通过从深度学习模型中移除特定数据点或概念来提升隐私保护并防止生成敏感内容。对抗性提示可以利用已忘记的模型生成包含被移除概念的内容，构成重大安全风险。然而，现有的对抗攻击方法在生成符合攻击者意图的内容时仍面临挑战，并且需要高昂的计算成本来寻找成功的提示。为了解决这些问题，我们提出了 ZIUM，即针对已忘记模型的零样本意图感知对抗攻击，它能够灵活定制目标攻击图像以体现攻击者的意图。此外，ZIUM 支持零样本对抗攻击，无需对先前已攻击的忘记概念进行额外优化。对多种 MU 场景的评估表明，ZIUM 在成功根据用户意图提示定制内容方面表现出色，攻击成功率优于现有方法。同时，其零样本对抗攻击显著降低了对先前已攻击的忘记概念的攻击时间。
SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders,"Jiahui Geng, Qing Li","Unlearning methods for vision-language models (VLMs) have primarily adapted techniques from large language models (LLMs), relying on weight updates that demand extensive annotated forget sets. Moreover, these methods perform unlearning at a coarse granularity, often leading to excessive forgetting and reduced model utility. To address this issue, we introduce SAUCE, a novel method that leverages sparse autoencoders (SAEs) for fine-grained and selective concept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture high-dimensional, semantically rich sparse features. It then identifies the features most relevant to the target concept for unlearning. During inference, it selectively modifies these features to suppress specific concepts while preserving unrelated information. We evaluate SAUCE on two distinct VLMs, LLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks: concrete concept unlearning (objects and sports scenes) and abstract concept unlearning (emotions, colors, and materials), encompassing a total of 60 concepts. Extensive experiments demonstrate that SAUCE outperforms state-of-the-art methods by 18.04% in unlearning quality while maintaining comparable model utility. Furthermore, we investigate SAUCE's robustness against widely used adversarial attacks, its transferability across models, and its scalability in handling multiple simultaneous unlearning requests. Our findings establish SAUCE as an effective and scalable solution for selective concept unlearning in VLMs.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Geng_SAUCE_Selective_Concept_Unlearning_in_Vision-Language_Models_with_Sparse_Autoencoders_ICCV_2025_paper.html,0.9140625,,视觉语言模型（VLM）的去学习方法主要借鉴了大型语言模型（LLM）的技术，依赖于需要大量标注遗忘集合的权重更新。此外，这些方法在粗粒度上进行去学习，往往导致过度遗忘并降低模型效用。为了解决这一问题，我们提出了 SAUCE，这是一种利用稀疏自编码器（SAE）实现细粒度、选择性概念去学习的新方法。简而言之，SAUCE 首先训练 SAE 以捕获高维、语义丰富的稀疏特征，然后识别与待去学习目标概念最相关的特征。在推理阶段，它选择性地修改这些特征，以抑制特定概念，同时保留无关信息。我们在两种不同的 VLM（LLaVA‑v1.5‑7B 和 LLaMA‑3.2‑11B‑Vision‑Instruct）上进行评估，涉及两类任务：具体概念去学习（对象和体育场景）和抽象概念去学习（情感、颜色和材料），共计 60 个概念。大量实验表明，SAUCE 在去学习质量上比最先进的方法提升了 18.04%，且保持了相当的模型效用。此外，我们还研究了 SAUCE 对常用对抗攻击的鲁棒性、跨模型的可迁移性以及在处理多个同步去学习请求时的可扩展性。研究结果表明，SAUCE 是一种有效且可扩展的 VLM 选择性概念去学习解决方案。
DADet: Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection,"Hongwei Yu, Xinlong Ding, Jiawei Li, Jinlong Wang, Yudong Zhang, Rongquan Wang, Huimin Ma, Jiansheng Chen","While image conditional diffusion models demonstrate impressive generation capabilities, they exhibit high vulnerability when facing backdoor and adversarial attacks. In this paper, we define a scenario named diffusion anomaly where the generated results of a reverse process under attack deviate significantly from the normal ones. By analyzing the underlying formation mechanism of the diffusion anomaly, we reveal how perturbations are amplified during the reverse process and accumulated in the results. Based on the analysis, we reveal the phenomena of divergence and homogeneity, which cause the diffusion process to deviate significantly from the normal process and to decline in diversity. Leveraging these two phenomena, we propose a method named Diffusion Anomaly Detection (DADet) to effectively detect both backdoor and adversarial attacks. Extensive experiments demonstrate that our proposal achieves excellent defense performance against backdoor and adversarial attacks. Specifically, for the backdoor attack detection, our method achieves an F1 score of 99% on different datasets, including MS COCO and CIFAR-10. For the detection of adversarial samples, the F1 score exceeds 84% across three adversarial attacks and two different tasks, evaluated on the MS COCO and Places365 datasets, respectively.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor_ICCV_2025_paper.html,0.90625,DADet：通过扩散异常检测保护图像条件扩散模型免受对抗性和后门攻击。,尽管图像条件扩散模型展示了令人印象深刻的生成能力，但在面对后门攻击和对抗性攻击时表现出高度脆弱性。本文定义了一种称为扩散异常的情景，即在攻击下逆过程生成的结果与正常结果出现显著偏离。通过分析扩散异常的根本形成机制，我们揭示了扰动在逆过程中的放大方式以及在结果中的累积效应。基于此分析，我们发现了发散性和同质性两种现象，这两者导致扩散过程显著偏离正常过程并且多样性下降。利用这两种现象，我们提出了一种名为Diffusion Anomaly Detection（DADet）的方法，能够有效检测后门攻击和对抗性攻击。大量实验表明，我们的方案在防御后门攻击和对抗性攻击方面表现出色。具体而言，在后门攻击检测方面，我们的方法在包括MS COCO和CIFAR-10在内的不同数据集上实现了99%的F1得分；在对抗样本检测方面，F1得分在三种对抗攻击和两种不同任务上均超过84%，分别在MS COCO和Places365数据集上进行评估。
TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models,"Ruidong Chen, Honglin Guo, Lanjun Wang, Chenyu Zhang, Weizhi Nie, An-An Liu","Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Chen_TRCE_Towards_Reliable_Malicious_Concept_Erasure_in_Text-to-Image_Diffusion_Models_ICCV_2025_paper.html,0.87890625,TRCE：朝向在文本到图像扩散模型中实现可靠的恶意概念消除。,近期文本到图像扩散模型的进展使得生成逼真图像成为可能，但也带来了生成恶意内容（如不安全内容）的风险。为降低风险，研究者们开始探索概念擦除方法，以促使模型忘记特定概念。然而，现有研究在完全擦除隐含在提示中的恶意概念（例如隐喻表达或对抗性提示）的同时，仍难以保持模型的正常生成能力。为了解决这一难题，我们的研究提出了TRCE，采用两阶段概念擦除策略，实现可靠擦除与知识保留之间的有效平衡。首先，TRCE从擦除文本提示中隐含的恶意语义入手。通过识别关键映射目标（即[EoT]嵌入），我们优化交叉注意力层，使恶意提示映射到语义相近但包含安全概念的提示，从而防止模型在去噪过程中受到恶意语义的过度影响。随后，考虑到扩散模型采样轨迹的确定性特征，TRCE进一步通过对比学习将早期去噪预测引导向安全方向，远离不安全方向，进一步避免生成恶意内容。最后，我们在多个恶意概念擦除基准上对TRCE进行全面评估，结果表明其在擦除恶意概念的同时，更好地保留了模型原有的生成能力。
SEAL: Semantic Aware Image Watermarking,"Kasra Arabi, R. Teal Witter, Chinmay Hegde, Niv Cohen","Generative models have rapidly evolved to generate realistic outputs. However, their synthetic outputs increasingly challenge the clear distinction between natural and AI-generated content, necessitating robust watermarking techniques to mark synthetic images. Watermarks are typically expected to preserve the integrity of the target image, withstand removal attempts, and prevent unauthorized insertion of the watermark pattern onto unrelated images. To address this need, recent methods embed persistent watermarks into images produced by diffusion models using the initial noise of the diffusion process. Yet, to do so, they either distort the distribution of generated images or require searching a large dictionary of candidate noise patterns for detection. In this paper, we propose a novel watermarking method that embeds semantic information about the generated image into the noise pattern, enabling a distortion-free watermark that can be verified without requiring a database of key patterns. Instead, the key pattern can be inferred from the semantic embedding of the image using locality-sensitive hashing. Furthermore, conditioning the watermark detection on the original image content improves its robustness against forgery attacks. To demonstrate that, we consider two largely overlooked attack strategies: (i) an attacker extracting the initial noise and generating a novel image with the same pattern; (ii) an attacker inserting an unrelated (potentially harmful) object into a watermarked image, while preserving the watermark. We empirically validate our method's increased robustness to these attacks. Taken together, our results suggest that content-aware watermarks can mitigate risks arising from image-generative models.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Arabi_SEAL_Semantic_Aware_Image_Watermarking_ICCV_2025_paper.html,0.87890625,,生成模型迅速发展，能够生成逼真的输出。然而，它们的合成结果日益模糊了自然内容与人工智能生成内容之间的界限，因而需要强健的水印技术来标记合成图像。水印通常要求保持目标图像的完整性，抵御去除尝试，并防止将水印图案未经授权地插入到无关图像中。为满足这一需求，近期方法利用扩散过程的初始噪声在扩散模型生成的图像中嵌入持久水印。但这样做要么会扭曲生成图像的分布，要么需要在检测时搜索庞大的候选噪声模式字典。本文提出一种新颖的水印方法，将生成图像的语义信息嵌入噪声模式，实现无失真的水印，并且无需关键模式数据库即可进行验证。相反，关键模式可以通过对图像的语义嵌入使用局部敏感哈希进行推断。进一步地，将水印检测条件化于原始图像内容，可提升其对伪造攻击的鲁棒性。为此，我们考虑了两种长期被忽视的攻击策略：(i) 攻击者提取初始噪声并利用相同模式生成新图像；(ii) 攻击者在带水印的图像中插入无关（可能有害）的对象，同时保持水印完整。我们通过实验验证了该方法在面对这些攻击时的增强鲁棒性。综合来看，我们的结果表明，内容感知水印能够降低图像生成模型带来的风险。
"Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design","Yuhao Sun, Yihua Zhang, Gaowen Liu, Hongtao Xie, Sijia Liu","With the increasing demand for the right to be forgotten, machine unlearning (MU) has emerged as a vital tool for enhancing trust and regulatory compliance by enabling the removal of sensitive data influences from machine learning (ML) models. However, most MU algorithms primarily rely on in-training methods to adjust model weights, with limited exploration of the benefits that data-level adjustments could bring to the unlearning process. To address this gap, we propose a novel approach that leverages digital watermarking to facilitate MU by strategically modifying data content. By integrating watermarking, we establish a controlled unlearning mechanism that enables precise removal of specified data while maintaining model utility for unrelated tasks. We first examine the impact of watermarked data on MU, finding that MU effectively generalizes to watermarked data. Building on this, we introduce an unlearning-friendly watermarking framework, termed Water4MU, to enhance unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO) framework: at the upper level, the watermarking network is optimized to minimize unlearning difficulty, while at the lower level, the model itself is trained independently of watermarking. Experimental results demonstrate that Water4MU is effective in MU across both image classification and image generation tasks. Notably, it outperforms existing methods in challenging MU scenarios, known as ""challenging forgets"".",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Invisible_Watermarks_Visible_Gains_Steering_Machine_Unlearning_with_Bi-Level_Watermarking_ICCV_2025_paper.html,0.8671875,隐形水印，显著收益：利用双层水印设计引导机器遗忘,随着对“被遗忘权”需求的增加，机器遗忘（MU）已成为提升信任和合规性的关键工具，它通过从机器学习（ML）模型中移除敏感数据的影响来实现。然而，大多数MU算法主要依赖于训练期间的方法来调整模型权重，对数据层面的调整在遗忘过程中的潜在益处探索有限。为弥补这一空白，我们提出一种利用数字水印技术通过策略性修改数据内容来促进MU的创新方法。通过引入水印，我们建立了一种受控的遗忘机制，能够精准删除指定数据，同时保持模型在无关任务上的效用。我们首先考察了带水印数据对MU的影响，发现MU能够有效地推广到带水印的数据上。在此基础上，我们提出了一个友好遗忘的水印框架，称为Water4MU，以提升遗忘效果。Water4MU的核心是双层优化（BLO）框架：上层优化水印网络以最小化遗忘难度，下层则独立于水印训练模型本身。实验结果表明，Water4MU在图像分类和图像生成任务中均能有效实现MU。值得注意的是，它在所谓的“挑战性遗忘”场景中优于现有方法。
Unlearning the Noisy Correspondence Makes CLIP More Robust,"Haochen Han, Alex Jinpeng Wang, Peijun Ye, Fangming Liu","The data appetite for Vision-Language Models (VLMs) has continuously scaled up from the early millions to billions today, which faces an untenable trade-off with data quality and inevitably introduces Noisy Correspondence (NC) samples. Undoubtedly, such semantically unrelated data significantly impairs the performance of VLMs. Previous efforts mainly address this challenge by estimating refined alignment for more precise guidance. However, such resource-intensive pipelines that train VLMs from scratch struggle to meet realistic data demands. In this paper, we present a brand new perspective that seeks to directly eliminate the harmful effects of NC in pre-trained VLMs. Specifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning framework that efficiently enhances VLMs' robustness by forgetting learned noisy knowledge. The key to NCU is learning the hardest negative information, which can provide explicit unlearning direction for both false positives and false negatives. Such twin goals unlearning process can be formalized into one unified optimal transport objective for fast fine-tuning. We validate our approach with the prevailing CLIP model over various downstream tasks. Remarkably, NCU surpasses the robust pre-trained method on zero-shot transfer while with lower computational overhead. The code is available at https://github.com/hhc1997/NCU.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Han_Unlearning_the_Noisy_Correspondence_Makes_CLIP_More_Robust_ICCV_2025_paper.html,0.8359375,消除噪声对应提升 CLIP 的鲁棒性,Vision-Language模型（VLM）的数据需求从早期的几百万规模不断扩大到如今的数十亿，这与数据质量之间形成了难以平衡的矛盾，必然会引入噪声对应（Noisy Correspondence，NC）样本。毫无疑问，这类语义不相关的数据会显著削弱VLM的性能。以往的工作主要通过估计更精细的对齐来提供更准确的指导，但这些从头训练VLM的资源密集型流程难以满足实际的数据需求。本文提出一种全新的视角，直接消除预训练VLM中NC的有害影响。具体而言，我们提出了NCU（Noisy Correspondence Unlearning）微调框架，通过遗忘已学习的噪声知识，高效提升VLM的鲁棒性。NCU的核心在于学习最困难的负样本信息，为假阳性和假阴性提供明确的遗忘方向。这一双重目标的遗忘过程可以形式化为统一的最优传输目标，从而实现快速微调。我们在主流的CLIP模型上进行验证，覆盖多种下游任务。令人惊讶的是，NCU在零样本迁移任务上超越了已有的鲁棒预训练方法，同时计算开销更低。代码已公开于 https://github.com/hhc1997/NCU。
ROAR: Reducing Inversion Error in Generative Image Watermarking,"Hanyi Wang, Han Fang, Shi-Lin Wang, Ee-Chien Chang","Generative image watermarking enables the proactive detection and traceability of generated images. Among existing methods, inversion-based frameworks achieve highly conceal ed watermark embedding by injecting watermarks into the latent representation before the diffusion process. The robustness of this approach hinges on both the embedding mechanism and inversion accuracy. However, prior works have predominantly focused on optimizing the embedding process while overlooking inversion errors, which significantly affect extraction fidelity. In this paper, we address the challenge of inversion errors and propose ROAR, a dual-domain optimization-based framework designed to mitigate errors arising from two key sources: 1) Latent-domain errors, which accumulate across inversion steps due to inherent approximation assumptions. 2) Pixel-domain errors, which result from channel distortions such as JPEG compression. To tackle these issues, we introduce two novel components: A Regeneration-based Optimization (RO) mechanism, which incorporates an optimizable starting latent to minimize latent-domain errors; A Mixture of Experts (MoE)-based distortion-adaptive restoration (AR) network, which effectively recovers watermarked distributions from pixel-level distortions.Extensive experiments demonstrate that ROAR significantly reduces inversion errors and enhances watermark extraction robustness, thereby improving the reliability of generative image watermarking.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking_ICCV_2025_paper.html,0.8359375,ROAR：在生成图像水印中降低反演误差,生成式图像水印技术实现了对生成图像的主动检测和可追溯性。在现有方法中，基于逆向的框架通过在扩散过程之前将水印注入潜在表示，实现了高度隐蔽的水印嵌入。这种方法的鲁棒性取决于嵌入机制和逆向精度。然而，以往工作主要关注优化嵌入过程，忽视了逆向误差，而逆向误差会显著影响提取的忠实度。本文针对逆向误差的挑战，提出了ROAR——一种基于双域优化的框架，旨在减轻来自两个关键来源的误差：1）潜在域误差，由于固有的近似假设，在逆向步骤中累积；2）像素域误差，由JPEG压缩等通道失真引起。为解决这些问题，我们引入了两个新颖组件：一种基于再生成的优化（RO）机制，利用可优化的起始潜在向量来最小化潜在域误差；一种基于专家混合（MoE）的失真自适应恢复（AR）网络，能够有效从像素级失真中恢复水印分布。大量实验表明，ROAR显著降低了逆向误差，提升了水印提取的鲁棒性，从而提高了生成式图像水印的可靠性。
DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models,"Seunghoo Hong, Geonho Son, Juhun Lee, Simon S. Woo","Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive performance. In this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community. Our code is available here: https://anonymous.4open.science/r/DIA-13419/.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Hong_DIA_The_Adversarial_Exposure_of_Deterministic_Inversion_in_Diffusion_Models_ICCV_2025_paper.html,0.796875,DIA：对扩散模型中确定性反演的对抗性曝光。,扩散模型已被证明是强大的表征学习器，在多个领域展示了最先进的性能。除了加速采样之外，DDIM 还能够将真实图像逆向映射回其潜在编码。该逆向操作的直接继承应用是真实图像编辑，其中逆向过程产生的潜在轨迹可在编辑图像的合成过程中使用。不幸的是，这一实用工具使得恶意用户能够更轻松地合成误导性或深度伪造内容，从而促进了不道德、侵权以及侵犯隐私和版权的内容传播。虽然诸如 AdvDM 和 Photoguard 等防御算法已被证明能够扰乱这些图像的扩散过程，但它们的目标与测试时的迭代去噪轨迹之间的不匹配导致了扰乱效果较弱。在本工作中，我们提出了 DDIM 逆向攻击（DIA），针对集成的 DDIM 轨迹路径进行攻击。实验结果表明，该方法能够有效扰乱，并在各种编辑方法上超越了之前的防御手段。我们相信，我们的框架和结果能够为行业和科研社区提供针对 AI 恶意使用的实用防御方案。我们的代码已公开：https://anonymous.4open.science/r/DIA-13419/。
TrustMark: Robust Watermarking and Watermark Removal for Arbitrary Resolution Images,"Tu Bui, Shruti Agarwal, John Collomosse","Imperceptible digital watermarking is important in copyright protection, misinformation prevention, and responsible generative AI. We propose TrustMark - a watermarking method that leverages a spatio-spectral loss function and a 1x1 convolution layer to enhance encoding quality. TrustMark is robust against both in-place and out-of-place perturbations while maintaining image quality above 43 dB. Additionally, we propose ReMark, a watermark removal method designed for re-watermarking, along with a simple yet effective algorithm that enables both TrustMark and ReMark to operate across arbitrary resolutions. Our methods achieve state-of-art performance on 3 benchmarks.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Bui_TrustMark_Robust_Watermarking_and_Watermark_Removal_for_Arbitrary_Resolution_Images_ICCV_2025_paper.html,0.796875,TrustMark：针对任意分辨率图像的鲁棒水印嵌入与去除。,不可感知的数字水印在版权保护、误信息防范以及负责任的生成式人工智能中具有重要意义。我们提出了TrustMark——一种利用时空频谱损失函数和1×1卷积层来提升编码质量的水印方法。TrustMark在保持图像质量高于43 dB的同时，对就地和非就地扰动均表现出强鲁棒性。此外，我们还提出了ReMark，这是一种专为重新水印设计的水印去除方法，并配备了一种简洁而有效的算法，使TrustMark和ReMark能够在任意分辨率下运行。我们的技术在三个基准测试中实现了最先进的性能。
Moderating the Generalization of Score-based Generative Model,"Wan Jiang, He Wang, Xin Zhang, Dan Guo, Zhaoxin Fan, Yunfeng Diao, Richang Hong","Score-based Generative Models (SGMs) have demonstrated remarkable generalization capabilities, e.g. generating unseen, but natural data. However, the greater the generalization power, the more likely the unintended generalization, and the more dangerous the abuse. Despite these concerns, research on unlearning SGMs has not been explored. To fill this gap, we first examine the current `gold standard' in Machine Unlearning (MU), i.e., re-training the model after removing the undesirable training data, and find it does not work in SGMs. Further analysis of score functions reveals that the MU 'gold standard' does not alter the original score function, which explains its ineffectiveness. Building on this insight, we propose the first Moderated Score-based Generative Model (MSGM), which introduces a novel score adjustment strategy that redirects the score function away from undesirable data during the continuous-time stochastic differential equation process. Albeit designed for SGMs, MSGM is a general and flexible MU framework compatible with diverse diffusion architectures, training strategies and downstream tasks. Code is available at https://github.com/yunfengdiao/Moderated-Score-based-Generative-Model.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Moderating_the_Generalization_of_Score-based_Generative_Model_ICCV_2025_paper.html,0.796875,,基于分数的生成模型（SGMs）展示了显著的泛化能力，例如能够生成未见但自然的数据。然而，泛化能力越强，意外泛化的可能性就越大，滥用的风险也越高。尽管存在这些担忧，针对SGMs的遗忘研究尚未被探索。为填补这一空白，我们首先考察了机器遗忘（MU）领域的“黄金标准”，即在移除不良训练数据后重新训练模型，发现该方法在SGMs中并不起作用。对分数函数的进一步分析表明，MU的“黄金标准”并未改变原始的分数函数，这解释了其无效性。基于此洞见，我们提出了首个受控分数生成模型（MSGM），该模型引入了一种新颖的分数调整策略，在连续时间随机微分方程过程中将分数函数从不良数据上转移。虽然MSGM是为SGMs设计的，但它是一个通用且灵活的MU框架，兼容多种扩散架构、训练策略和下游任务。代码已在 https://github.com/yunfengdiao/Moderated-Score-based-Generative-Model 发布。
AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs,"Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha","With the rapid advancement of Multi-modal Large Language Models (MLLMs), several diagnostic benchmarks have recently been developed to assess these models' multimodal reasoning proficiency. However, these benchmarks are restricted to assessing primarily the visual aspect and do not examine the holistic audio-visual (AV) understanding. Moreover, currently, there are no benchmarks that investigate the capabilities of AVLLMs to calibrate their responses when presented with perturbed inputs. To this end, we introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial Attack, Compositional Reasoning, and Modality-specific Dependency. Using our benchmark, we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Chowdhury_AVTrustBench_Assessing_and_Enhancing_Reliability_and_Robustness_in_Audio-Visual_LLMs_ICCV_2025_paper.html,0.796875,AVTrustBench：评估与提升音视频大语言模型的可靠性与鲁棒性,随着多模态大型语言模型（MLLM）快速发展，近期出现了若干诊断基准用于评估这些模型的多模态推理能力。然而，这些基准主要局限于对视觉方面的评估，未能考察整体的音视频（AV）理解能力。此外，目前尚无基准能够研究音视频大语言模型（AVLLM）在面对扰动输入时校准其响应的能力。为此，我们推出了音视频可信度评估基准（AVTrustBench），该基准包含60万条样本，覆盖9个精心设计的任务，评估AVLLM在对抗攻击、组合推理和模态特定依赖三个维度上的能力。基于该基准，我们对16种最先进的AVLLM进行了广泛评测，结果显示大多数现有模型距离人类水平的理解仍有显著差距，为未来研究方向提供了宝贵的洞见。为克服现有方法的局限，我们进一步提出了一种鲁棒的、模型无关的校准音视频偏好优化训练策略（CAVPref），在全部9个任务上实现了最高30.19%的性能提升。
